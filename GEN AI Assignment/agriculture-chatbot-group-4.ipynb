{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-25T11:59:52.890287Z",
     "iopub.status.busy": "2025-04-25T11:59:52.890054Z",
     "iopub.status.idle": "2025-04-25T12:02:58.858105Z",
     "shell.execute_reply": "2025-04-25T12:02:58.857308Z",
     "shell.execute_reply.started": "2025-04-25T11:59:52.890258Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unsloth in c:\\users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages (2025.4.3)\n",
      "Collecting git+https://github.com/unslothai/unsloth.git\n",
      "  Cloning https://github.com/unslothai/unsloth.git to c:\\users\\ayush\\appdata\\local\\temp\\pip-req-build-qfp98kic\n",
      "  Resolved https://github.com/unslothai/unsloth.git to commit 8a055402a27c3d9643cc16947ce40311a280e69c\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: unsloth\n",
      "  Building wheel for unsloth (pyproject.toml): started\n",
      "  Building wheel for unsloth (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for unsloth: filename=unsloth-2025.4.8-py3-none-any.whl size=265064 sha256=c0815f2647b5284f1bfb6951345efefd9da41086194cd7a1a82a04f0de2854bb\n",
      "  Stored in directory: C:\\Users\\ayush\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-bt70xzl7\\wheels\\0b\\bf\\f5\\61523189908a01bce8752a181f02f8b057ffc2c792447d39ff\n",
      "Successfully built unsloth\n",
      "Installing collected packages: unsloth\n",
      "  Attempting uninstall: unsloth\n",
      "    Found existing installation: unsloth 2025.4.3\n",
      "    Uninstalling unsloth-2025.4.3:\n",
      "      Successfully uninstalled unsloth-2025.4.3\n",
      "Successfully installed unsloth-2025.4.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/unslothai/unsloth.git 'C:\\Users\\ayush\\AppData\\Local\\Temp\\pip-req-build-qfp98kic'\n"
     ]
    }
   ],
   "source": [
    "# !pip install unsloth\n",
    "# !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:02:58.860499Z",
     "iopub.status.busy": "2025-04-25T12:02:58.860229Z",
     "iopub.status.idle": "2025-04-25T12:03:35.271550Z",
     "shell.execute_reply": "2025-04-25T12:03:35.270893Z",
     "shell.execute_reply.started": "2025-04-25T12:02:58.860475Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'triton'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer \n",
      "File \u001b[1;32mc:\\Users\\ayush\\anaconda3\\envs\\tf\\lib\\site-packages\\unsloth\\__init__.py:150\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# For Gradio HF Spaces?\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# if \"SPACE_AUTHOR_NAME\" not in os.environ and \"SPACE_REPO_NAME\" not in os.environ:\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtriton\u001b[39;00m\n\u001b[0;32m    151\u001b[0m libcuda_dirs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Version(triton\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m Version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'triton'"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch \n",
    "from trl import SFTTrainer \n",
    "from unsloth import is_bfloat16_supported \n",
    "from huggingface_hub import login \n",
    "from transformers import TrainingArguments \n",
    "from datasets import load_dataset \n",
    "import wandb\n",
    "from kaggle_secrets import UserSecretsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:05:05.202844Z",
     "iopub.status.busy": "2025-04-25T12:05:05.202196Z",
     "iopub.status.idle": "2025-04-25T12:05:17.633990Z",
     "shell.execute_reply": "2025-04-25T12:05:17.633412Z",
     "shell.execute_reply.started": "2025-04-25T12:05:05.202816Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "user_secrets = UserSecretsClient() \n",
    "hugging_face_token = user_secrets.get_secret(\"Hugging_Face_Token\")\n",
    "wnb_token = user_secrets.get_secret(\"wnb\")\n",
    "\n",
    "login(hugging_face_token) \n",
    "\n",
    "wandb.login(key=wnb_token) \n",
    "run = wandb.init(\n",
    "    project='', \n",
    "    job_type=\"training\", \n",
    "    anonymous=\"allow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:05:45.961536Z",
     "iopub.status.busy": "2025-04-25T12:05:45.961167Z",
     "iopub.status.idle": "2025-04-25T12:06:14.629272Z",
     "shell.execute_reply": "2025-04-25T12:06:14.628693Z",
     "shell.execute_reply.started": "2025-04-25T12:05:45.961512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",  \n",
    "    max_seq_length=max_seq_length, \n",
    "    dtype=dtype, \n",
    "    load_in_4bit=load_in_4bit, \n",
    "    token=hugging_face_token, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:06:27.213296Z",
     "iopub.status.busy": "2025-04-25T12:06:27.212729Z",
     "iopub.status.idle": "2025-04-25T12:06:27.218027Z",
     "shell.execute_reply": "2025-04-25T12:06:27.217350Z",
     "shell.execute_reply.started": "2025-04-25T12:06:27.213267Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and provide a step-by-step chain of reasoning to ensure a comprehensive and scientifically grounded response.\n",
    "\n",
    "### Instruction:\n",
    "You are an agricultural expert with deep knowledge in crop science, soil management, sustainable farming practices, agri-tech innovations, and climate-resilient agriculture. \n",
    "Please answer the following agriculture-related question.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:09:20.692496Z",
     "iopub.status.busy": "2025-04-25T12:09:20.692172Z",
     "iopub.status.idle": "2025-04-25T12:09:58.128143Z",
     "shell.execute_reply": "2025-04-25T12:09:58.127237Z",
     "shell.execute_reply.started": "2025-04-25T12:09:20.692475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "question = \"\"\"How can I improve water efficiency in agriculture?\"\"\"\n",
    "\n",
    "FastLanguageModel.for_inference(model)  \n",
    "\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  \n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids, \n",
    "    attention_mask=inputs.attention_mask, \n",
    "    max_new_tokens=1200, \n",
    "    use_cache=True, \n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:10:43.316700Z",
     "iopub.status.busy": "2025-04-25T12:10:43.316324Z",
     "iopub.status.idle": "2025-04-25T12:10:43.322567Z",
     "shell.execute_reply": "2025-04-25T12:10:43.321713Z",
     "shell.execute_reply.started": "2025-04-25T12:10:43.316678Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \n",
    "Write a response that appropriately completes the request. \n",
    "Before answering, think carefully about the question and provide a step-by-step chain of reasoning to ensure a comprehensive and scientifically grounded response.\n",
    "\n",
    "### Instruction:\n",
    "You are an agricultural expert with deep knowledge in crop science, soil management, sustainable farming practices, agri-tech innovations, and climate-resilient agriculture. \n",
    "Please answer the following agriculture-related question.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:11:00.226226Z",
     "iopub.status.busy": "2025-04-25T12:11:00.225923Z",
     "iopub.status.idle": "2025-04-25T12:11:01.181290Z",
     "shell.execute_reply": "2025-04-25T12:11:01.180697Z",
     "shell.execute_reply.started": "2025-04-25T12:11:00.226204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"KisanVaani/agriculture-qa-english-only\", split = \"train[0:500]\",trust_remote_code=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:11:12.126283Z",
     "iopub.status.busy": "2025-04-25T12:11:12.125987Z",
     "iopub.status.idle": "2025-04-25T12:11:12.135493Z",
     "shell.execute_reply": "2025-04-25T12:11:12.134755Z",
     "shell.execute_reply.started": "2025-04-25T12:11:12.126262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:11:27.388620Z",
     "iopub.status.busy": "2025-04-25T12:11:27.387987Z",
     "iopub.status.idle": "2025-04-25T12:11:27.394592Z",
     "shell.execute_reply": "2025-04-25T12:11:27.393838Z",
     "shell.execute_reply.started": "2025-04-25T12:11:27.388593Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  \n",
    "EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:11:32.405950Z",
     "iopub.status.busy": "2025-04-25T12:11:32.405657Z",
     "iopub.status.idle": "2025-04-25T12:11:32.412194Z",
     "shell.execute_reply": "2025-04-25T12:11:32.411412Z",
     "shell.execute_reply.started": "2025-04-25T12:11:32.405930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):  \n",
    "    inputs = examples[\"input\"]         \n",
    "    outputs = examples[\"response\"]      \n",
    "\n",
    "    texts = []  \n",
    "\n",
    "    for input, output in zip(inputs, outputs):  \n",
    "        text = train_prompt_style.format(input, output) + EOS_TOKEN  \n",
    "        texts.append(text)  \n",
    "\n",
    "    return {\n",
    "        \"text\": texts,  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:11:34.730134Z",
     "iopub.status.busy": "2025-04-25T12:11:34.729833Z",
     "iopub.status.idle": "2025-04-25T12:11:34.767042Z",
     "shell.execute_reply": "2025-04-25T12:11:34.766339Z",
     "shell.execute_reply.started": "2025-04-25T12:11:34.730112Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset_finetune = dataset.map(formatting_prompts_func, batched = True)\n",
    "dataset_finetune[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:12:01.495849Z",
     "iopub.status.busy": "2025-04-25T12:12:01.495225Z",
     "iopub.status.idle": "2025-04-25T12:12:08.642232Z",
     "shell.execute_reply": "2025-04-25T12:12:08.641422Z",
     "shell.execute_reply.started": "2025-04-25T12:12:01.495821Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_lora = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  \n",
    "    target_modules=[  \n",
    "        \"q_proj\",   \n",
    "        \"k_proj\",   \n",
    "        \"v_proj\",   \n",
    "        \"o_proj\",   \n",
    "        \"gate_proj\",  \n",
    "        \"up_proj\",    \n",
    "        \"down_proj\",  \n",
    "    ],\n",
    "    lora_alpha=16,  \n",
    "    lora_dropout=0,  \n",
    "    bias=\"none\",  \n",
    "    use_gradient_checkpointing=\"unsloth\",  \n",
    "    random_state=3407,  \n",
    "    use_rslora=False,  \n",
    "    loftq_config=None,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T12:12:16.308804Z",
     "iopub.status.busy": "2025-04-25T12:12:16.308500Z",
     "iopub.status.idle": "2025-04-25T12:12:18.924987Z",
     "shell.execute_reply": "2025-04-25T12:12:18.924337Z",
     "shell.execute_reply.started": "2025-04-25T12:12:16.308783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model_lora,  \n",
    "    tokenizer=tokenizer,  \n",
    "    train_dataset=dataset_finetune,  \n",
    "    dataset_text_field=\"text\",  \n",
    "    max_seq_length=max_seq_length,  \n",
    "    dataset_num_proc=2,  \n",
    "\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,  \n",
    "        gradient_accumulation_steps=4,  \n",
    "        num_train_epochs=1, \n",
    "        warmup_steps=5,  \n",
    "        max_steps=60,  \n",
    "        learning_rate=2e-4,  \n",
    "        fp16=not is_bfloat16_supported(),  \n",
    "        bf16=is_bfloat16_supported(),  \n",
    "        logging_steps=10,  \n",
    "        optim=\"adamw_8bit\",  \n",
    "        weight_decay=0.01,  \n",
    "        lr_scheduler_type=\"linear\",  \n",
    "        seed=3407,  \n",
    "        output_dir=\"outputs\",  \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T13:03:33.978838Z",
     "iopub.status.busy": "2025-04-25T13:03:33.978470Z",
     "iopub.status.idle": "2025-04-25T13:19:04.308415Z",
     "shell.execute_reply": "2025-04-25T13:19:04.307607Z",
     "shell.execute_reply.started": "2025-04-25T13:03:33.978815Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T13:19:58.644342Z",
     "iopub.status.busy": "2025-04-25T13:19:58.643959Z",
     "iopub.status.idle": "2025-04-25T13:19:58.650319Z",
     "shell.execute_reply": "2025-04-25T13:19:58.649767Z",
     "shell.execute_reply.started": "2025-04-25T13:19:58.644320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T13:21:52.895216Z",
     "iopub.status.busy": "2025-04-25T13:21:52.894588Z",
     "iopub.status.idle": "2025-04-25T13:21:59.049355Z",
     "shell.execute_reply": "2025-04-25T13:21:59.048635Z",
     "shell.execute_reply.started": "2025-04-25T13:21:52.895190Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "question = \"\"\"What are some methods for controlling pests and diseases in greenhouse tomato production?\"\"\"\n",
    "\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model_lora.generate(\n",
    "    input_ids=inputs.input_ids,          \n",
    "    attention_mask=inputs.attention_mask, \n",
    "    max_new_tokens=1200,                  \n",
    "    use_cache=True,                        \n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T13:22:28.499851Z",
     "iopub.status.busy": "2025-04-25T13:22:28.499147Z",
     "iopub.status.idle": "2025-04-25T13:22:37.943631Z",
     "shell.execute_reply": "2025-04-25T13:22:37.942789Z",
     "shell.execute_reply.started": "2025-04-25T13:22:28.499826Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "from peft import PeftModel\n",
    "import os\n",
    "repo_name = \"NamanSharma15/Agricultural-Finetuned-LLama-3.1-8B\"\n",
    "tokenizer.push_to_hub(repo_name)\n",
    "model_lora.push_to_hub(repo_name)\n",
    "\n",
    "print(f\"Successfully pushed LoRA model and tokenizer to: https://huggingface.co/{repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T13:22:46.653439Z",
     "iopub.status.busy": "2025-04-25T13:22:46.652906Z",
     "iopub.status.idle": "2025-04-25T13:23:00.871051Z",
     "shell.execute_reply": "2025-04-25T13:23:00.870231Z",
     "shell.execute_reply.started": "2025-04-25T13:22:46.653409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_seq_length = 2048 \n",
    "dtype = None \n",
    "load_in_4bit = True \n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"NamanSharma15/Agricultural-Finetuned-LLama-3.1-8B\",  \n",
    "    max_seq_length=max_seq_length, \n",
    "    dtype=dtype, \n",
    "    load_in_4bit=load_in_4bit, \n",
    "    token=hugging_face_token, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-25T13:23:33.667801Z",
     "iopub.status.busy": "2025-04-25T13:23:33.667479Z",
     "iopub.status.idle": "2025-04-25T13:23:40.189505Z",
     "shell.execute_reply": "2025-04-25T13:23:40.188776Z",
     "shell.execute_reply.started": "2025-04-25T13:23:33.667779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "question = \"\"\"How can I improve water efficiency in agriculture?\"\"\" \n",
    "\n",
    "inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")  \n",
    "\n",
    "outputs = model_lora.generate(\n",
    "    input_ids=inputs.input_ids, \n",
    "    attention_mask=inputs.attention_mask, \n",
    "    max_new_tokens=1200, \n",
    "    use_cache=True, \n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
